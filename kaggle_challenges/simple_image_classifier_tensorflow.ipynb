{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "#tf.VERSION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/sneha/personal_stuff/dl_begin/dl_beginner/meta-data/train.csv')\n",
    "test = pd.read_csv('/home/sneha/personal_stuff/dl_begin/dl_beginner/meta-data/test.csv')\n",
    "TRAIN_PATH = '/home/sneha/personal_stuff/dl_begin/dl_beginner/train/' \n",
    "TEST_PATH = '/home/sneha/personal_stuff/dl_begin/dl_beginner/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_id</th>\n",
       "      <th>Animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Img-1.jpg</td>\n",
       "      <td>hippopotamus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Img-2.jpg</td>\n",
       "      <td>squirrel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Img-3.jpg</td>\n",
       "      <td>grizzly+bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Img-4.jpg</td>\n",
       "      <td>ox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Img-5.jpg</td>\n",
       "      <td>german+shepherd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Image_id           Animal\n",
       "0  Img-1.jpg     hippopotamus\n",
       "1  Img-2.jpg         squirrel\n",
       "2  Img-3.jpg     grizzly+bear\n",
       "3  Img-4.jpg               ox\n",
       "4  Img-5.jpg  german+shepherd"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13000 images in the train dataset.\n",
      "There are 6000 images in the test datasets.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} images in the train dataset.'.format(train.shape[0]))\n",
    "print('There are {} images in the test datasets.'.format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contain images of the following animals:\n",
      "\n",
      " ['hippopotamus' 'squirrel' 'grizzly+bear' 'ox' 'german+shepherd' 'buffalo'\n",
      " 'otter' 'bobcat' 'wolf' 'persian+cat' 'collie' 'antelope' 'seal'\n",
      " 'dalmatian' 'siamese+cat' 'moose' 'horse' 'killer+whale' 'mouse' 'walrus'\n",
      " 'beaver' 'rhinoceros' 'chimpanzee' 'weasel' 'spider+monkey' 'raccoon'\n",
      " 'rat' 'chihuahua' 'mole' 'bat']\n"
     ]
    }
   ],
   "source": [
    "print('The dataset contain images of the following animals:\\n\\n', train.Animal.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize_images(image, [224, 224])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.read_file(path)\n",
    "    return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Image_id = TRAIN_PATH+train.Image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Image_id = TEST_PATH+test.Image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_ds = tf.data.Dataset.from_tensor_slices(train.Image_id)\n",
    "\n",
    "print('shape: ', repr(path_ds.output_shapes))\n",
    "print('type: ', path_ds.output_types)\n",
    "print()\n",
    "print(path_ds)\n",
    "\n",
    "image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=8)\n",
    "\n",
    "label_to_index = dict((name, index) for index,name in enumerate(train.Animal.unique()))\n",
    "print(label_to_index)\n",
    "\n",
    "label_ds = tf.data.Dataset.from_tensor_slices(tf.cast([label_to_index[i] for i in train.Animal], tf.int64))\n",
    "\n",
    "for label in label_ds.take(10):\n",
    "    print(train.Animal.iloc[label.numpy()])\n",
    "\n",
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "print('image shape: ', image_label_ds.output_shapes[0])\n",
    "print('label shape: ', image_label_ds.output_shapes[1])\n",
    "print('types: ', image_label_ds.output_types)\n",
    "print()\n",
    "print(image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "num_epochs = 10\n",
    "\n",
    "def train_input_fn():\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(train.Image_id)\n",
    "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=8)\n",
    "\n",
    "    label_to_index = dict((name, index) for index,name in enumerate(train.Animal.unique()))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(tf.cast([label_to_index[i] for i in train.Animal], tf.int64))\n",
    "    image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "    # Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
    "    # completely shuffled.\n",
    "    ds = image_label_ds.shuffle(buffer_size=train.shape[0])\n",
    "    ds = ds.repeat(num_epochs)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
    "    ds = ds.prefetch(buffer_size=BATCH_SIZE)\n",
    "    return ds.make_one_shot_iterator().get_next()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "num_epochs = 10\n",
    "\n",
    "def test_input_fn():\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(test.Image_id)\n",
    "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=8)\n",
    "    ds=image_ds\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
    "    ds = ds.prefetch(buffer_size=BATCH_SIZE)\n",
    "    return ds.make_one_shot_iterator().get_next()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features, [-1, 224, 224, 3])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=conv1,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "      inputs=conv2,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    dropout1 = tf.layers.dropout(\n",
    "      inputs=pool1, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(\n",
    "      inputs=dropout1,\n",
    "      filters=128,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(\n",
    "      inputs=conv4,\n",
    "      filters=128,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    conv6 = tf.layers.conv2d(\n",
    "      inputs=conv5,\n",
    "      filters=128,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv6, pool_size=[2, 2], strides=2)\n",
    "    print()\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1,56*56*128])\n",
    "    dropout2_flat = tf.layers.dropout(\n",
    "      inputs=pool2_flat, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    dense = tf.layers.dense(inputs=dropout2_flat, units=100, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=30)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    tf.summary.scalar('accuracy', eval_metric_ops['accuracy'])\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "modeldir = os.path.join(os.getcwd(), datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(modeldir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/sneha/personal_stuff/dl_begin/2019-01-27_23-32-10', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5a67e730b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create the Estimator\n",
    "animal_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=modeldir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\",\"loss\" : \"loss\"}\n",
    "\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/sneha/personal_stuff/dl_begin/2019-01-27_23-32-10/model.ckpt.\n",
      "INFO:tensorflow:loss = b'\\n\\x0b\\n\\x04loss\\x15\\xf1\\xe3Y@', probabilities = [[0.03282968 0.03134601 0.03300159 0.03403704 0.03221723 0.03502687\n",
      "  0.03392608 0.03190167 0.03563409 0.03275656 0.0345325  0.03535771\n",
      "  0.03323707 0.03380083 0.03445099 0.03452428 0.03334696 0.03428003\n",
      "  0.03464954 0.03110941 0.03107699 0.03267748 0.03302389 0.0318406\n",
      "  0.03389145 0.032053   0.03334408 0.03221411 0.03343787 0.03447441]\n",
      " [0.03109831 0.03265173 0.03288491 0.03262954 0.03241358 0.03616821\n",
      "  0.03405002 0.0348274  0.03329391 0.0341601  0.03242457 0.03563067\n",
      "  0.03374429 0.03557314 0.03202108 0.0347045  0.03324044 0.03298395\n",
      "  0.03298784 0.03269177 0.03212457 0.03425775 0.03149858 0.03149449\n",
      "  0.03262779 0.03397887 0.03369816 0.03252055 0.0352051  0.03241412]\n",
      " [0.03402525 0.03201275 0.0312722  0.03270857 0.03206357 0.03517682\n",
      "  0.03392011 0.03401244 0.03293432 0.03546861 0.03240426 0.03514997\n",
      "  0.03552654 0.03430495 0.03359228 0.03406192 0.03261057 0.03096909\n",
      "  0.03341436 0.03313394 0.03317639 0.0355231  0.03111181 0.03214081\n",
      "  0.03167203 0.03349598 0.03398871 0.03261004 0.03531217 0.03220657]\n",
      " [0.0304566  0.03148803 0.03166645 0.03357221 0.03187238 0.03578883\n",
      "  0.03360622 0.03558974 0.03298317 0.03308824 0.0340893  0.03488955\n",
      "  0.03451451 0.03231421 0.03501198 0.03377574 0.03222758 0.03252772\n",
      "  0.03289212 0.03116075 0.03119793 0.03452948 0.03267089 0.03174964\n",
      "  0.03374608 0.03526111 0.03573187 0.033275   0.03511031 0.03321233]\n",
      " [0.03195307 0.03306787 0.03327214 0.03122348 0.03237549 0.03525248\n",
      "  0.03476434 0.03179265 0.03517555 0.03215289 0.03318812 0.03314625\n",
      "  0.03169234 0.03292352 0.03530931 0.03598513 0.03630933 0.03272606\n",
      "  0.03534935 0.03233888 0.03190457 0.03335715 0.03215319 0.0314462\n",
      "  0.03318072 0.03328091 0.03473134 0.03159283 0.03327667 0.03507815]\n",
      " [0.03373333 0.03401677 0.03256753 0.03291106 0.03307705 0.0336824\n",
      "  0.03281375 0.03270075 0.03379162 0.0327138  0.03374016 0.03338036\n",
      "  0.03435839 0.03436875 0.03367573 0.03429108 0.03450218 0.03261983\n",
      "  0.03375103 0.03257804 0.03324433 0.03411512 0.03218695 0.0327352\n",
      "  0.03330428 0.03309258 0.03316361 0.03215455 0.03300935 0.03372049]\n",
      " [0.03306337 0.03289431 0.03234513 0.03213989 0.03285329 0.03403052\n",
      "  0.03343647 0.03252299 0.03427707 0.0329725  0.03350878 0.03387774\n",
      "  0.03299291 0.03368043 0.03373573 0.03438708 0.03453737 0.03336861\n",
      "  0.03360839 0.03303103 0.03343305 0.03389216 0.03230613 0.03306939\n",
      "  0.03267112 0.03259119 0.03382432 0.03365517 0.03404686 0.03324708]\n",
      " [0.03255927 0.03252243 0.03260288 0.03222293 0.03331146 0.03534367\n",
      "  0.03330254 0.03297861 0.033997   0.03302727 0.03381143 0.03381369\n",
      "  0.03286993 0.03289723 0.035117   0.03470577 0.03434705 0.03247323\n",
      "  0.03417308 0.0323351  0.0320648  0.03322827 0.03009137 0.03114868\n",
      "  0.03271194 0.03448251 0.03412737 0.03268689 0.03663876 0.03440781]\n",
      " [0.03413843 0.03301337 0.03379987 0.031291   0.03178236 0.0338136\n",
      "  0.03444602 0.0320068  0.03337699 0.03356728 0.03471681 0.03471754\n",
      "  0.03480759 0.03259594 0.03309903 0.03390022 0.03284138 0.03322306\n",
      "  0.03440284 0.03220068 0.03201396 0.03457706 0.03251536 0.03397211\n",
      "  0.03343435 0.03296109 0.03473882 0.03274789 0.03198578 0.03331265]\n",
      " [0.0318799  0.0325241  0.03245793 0.03258288 0.03136438 0.03386992\n",
      "  0.03283914 0.03538008 0.03145182 0.03448412 0.03481456 0.03373441\n",
      "  0.03419024 0.03575232 0.03142825 0.03267082 0.03382815 0.03364268\n",
      "  0.03337412 0.03249506 0.03244635 0.03627543 0.03392734 0.03308914\n",
      "  0.03210285 0.03285646 0.03546645 0.03312176 0.03429444 0.03165501]\n",
      " [0.03468507 0.03358277 0.03328663 0.03247985 0.03220565 0.03315299\n",
      "  0.03273191 0.03301173 0.03261113 0.03397741 0.03283184 0.03592392\n",
      "  0.03419087 0.03266072 0.03451217 0.03303756 0.03318421 0.03289531\n",
      "  0.03388939 0.03315605 0.03350223 0.03193124 0.03165579 0.03258146\n",
      "  0.03295677 0.03244999 0.03348947 0.03569827 0.03435429 0.03337325]\n",
      " [0.03266784 0.03402401 0.03257081 0.03179473 0.03303556 0.03507384\n",
      "  0.03306046 0.03476315 0.03601532 0.03247053 0.03175928 0.03452629\n",
      "  0.0343954  0.03533368 0.03234854 0.0340553  0.03552899 0.03257895\n",
      "  0.03428793 0.03349164 0.03274754 0.03371435 0.03224222 0.03154065\n",
      "  0.03090706 0.0329156  0.03235525 0.03288948 0.03489226 0.03201343]\n",
      " [0.03217697 0.03346925 0.03222653 0.03280284 0.03298591 0.03230966\n",
      "  0.03133617 0.03199382 0.0365659  0.0321203  0.03393919 0.03670322\n",
      "  0.03428382 0.03512662 0.03626885 0.03535594 0.03771332 0.03168703\n",
      "  0.03724105 0.03021083 0.03292843 0.03426015 0.03035562 0.02945105\n",
      "  0.03246167 0.03220417 0.0307392  0.03163909 0.03446305 0.03498048]\n",
      " [0.03266998 0.03176431 0.03240146 0.03221185 0.03261755 0.03531116\n",
      "  0.03343022 0.03489846 0.0335499  0.03275687 0.03453727 0.0345819\n",
      "  0.03658253 0.03357963 0.03453963 0.03355511 0.03346544 0.03349166\n",
      "  0.03369471 0.03163731 0.03260829 0.03415101 0.03217489 0.03180456\n",
      "  0.03243543 0.03407434 0.03417521 0.03170979 0.03273296 0.0328566 ]\n",
      " [0.03315649 0.03229388 0.03228692 0.03359334 0.03272448 0.03541659\n",
      "  0.03284278 0.035384   0.03321413 0.03273594 0.03364294 0.03358071\n",
      "  0.03293145 0.03570532 0.03448683 0.03494891 0.03358902 0.03351138\n",
      "  0.03235707 0.03178191 0.03162222 0.03205599 0.03178783 0.03075018\n",
      "  0.0327944  0.03315673 0.0340944  0.03334518 0.03538244 0.0348265 ]\n",
      " [0.03349853 0.03425051 0.03291415 0.03354456 0.03289488 0.03500535\n",
      "  0.03288652 0.03140905 0.0337646  0.03346018 0.03262213 0.03354881\n",
      "  0.03395121 0.03449506 0.0340373  0.03421813 0.03436958 0.03055216\n",
      "  0.03466647 0.03096473 0.03293647 0.03265769 0.03206387 0.03341531\n",
      "  0.0310556  0.03587278 0.03339348 0.03273862 0.03588712 0.03292507]\n",
      " [0.0330284  0.03285517 0.03235023 0.03299314 0.03203633 0.0368169\n",
      "  0.0330627  0.03319774 0.03615749 0.03413893 0.03405024 0.03542763\n",
      "  0.03263156 0.03338667 0.03547267 0.03576249 0.03441803 0.03338262\n",
      "  0.03266653 0.02978961 0.03014466 0.03361037 0.0300191  0.03067661\n",
      "  0.03159759 0.03271677 0.03485983 0.03266295 0.03704012 0.03304679]\n",
      " [0.03104375 0.03208349 0.03434144 0.03296215 0.03231256 0.03593614\n",
      "  0.03457341 0.03374897 0.03416054 0.03326267 0.03280088 0.03164579\n",
      "  0.03210334 0.0341684  0.03397966 0.03302507 0.03432903 0.03320798\n",
      "  0.03295734 0.03163511 0.03175603 0.03421099 0.03223902 0.03233647\n",
      "  0.03274889 0.03340978 0.0343511  0.03464951 0.03664134 0.03337906]\n",
      " [0.03302176 0.03286012 0.03481498 0.03190977 0.03324005 0.03448284\n",
      "  0.03334647 0.03413995 0.03301277 0.03475813 0.03408785 0.03379589\n",
      "  0.03418604 0.03373471 0.03265056 0.0331149  0.03333879 0.03382603\n",
      "  0.03224188 0.0329875  0.03249243 0.03436451 0.03264486 0.03290928\n",
      "  0.03267527 0.03222815 0.03332544 0.03340793 0.03419737 0.03220383]\n",
      " [0.03420229 0.03348535 0.03320177 0.03109626 0.03199707 0.03443563\n",
      "  0.03310259 0.03224569 0.0346022  0.03383102 0.0329973  0.03421566\n",
      "  0.0336092  0.03451011 0.03310753 0.03591258 0.03538705 0.03203652\n",
      "  0.03534824 0.03053647 0.03181418 0.0335427  0.03149303 0.03245072\n",
      "  0.03258356 0.0340448  0.03377714 0.03376196 0.03276152 0.03390991]\n",
      " [0.03213828 0.03135997 0.03446719 0.03291014 0.03223443 0.03422147\n",
      "  0.0356626  0.03344275 0.0327117  0.03404626 0.03403491 0.03318888\n",
      "  0.03281384 0.03370501 0.03361353 0.03389332 0.03237274 0.03448283\n",
      "  0.03470555 0.03341228 0.02962813 0.03367286 0.03168805 0.03222288\n",
      "  0.03566746 0.0325388  0.03382908 0.03372481 0.03420748 0.03340279]\n",
      " [0.0312756  0.03295155 0.03373222 0.03179264 0.03276944 0.0344229\n",
      "  0.0330934  0.03283631 0.03286342 0.03318726 0.03373061 0.03475854\n",
      "  0.03460504 0.03361729 0.03329789 0.03325728 0.03423055 0.03186685\n",
      "  0.03461083 0.03199721 0.03292611 0.03461155 0.0308155  0.03303172\n",
      "  0.03254059 0.03459695 0.03415379 0.03354361 0.03501204 0.0338714 ]\n",
      " [0.03309714 0.03157018 0.03232031 0.03219043 0.03423845 0.03524775\n",
      "  0.03489215 0.03222501 0.03371138 0.03304587 0.03257016 0.03671767\n",
      "  0.03417699 0.03522757 0.03313686 0.03520699 0.03359843 0.03038854\n",
      "  0.03449671 0.03280975 0.03442007 0.03253175 0.03110536 0.03154727\n",
      "  0.03138059 0.0335191  0.03410486 0.03238975 0.03550504 0.03262797]\n",
      " [0.03434275 0.03340437 0.03206415 0.03325331 0.03323004 0.0332281\n",
      "  0.03201269 0.03371606 0.03352403 0.0334831  0.03490699 0.03456802\n",
      "  0.03486907 0.03380231 0.03343223 0.03187033 0.03355511 0.03352284\n",
      "  0.03298607 0.0331924  0.03290075 0.03473127 0.03157979 0.032183\n",
      "  0.03325333 0.03425687 0.03244342 0.03228774 0.03338352 0.0340163 ]\n",
      " [0.03109816 0.03143601 0.03349016 0.0336917  0.03203447 0.03720119\n",
      "  0.03537576 0.03230383 0.03425655 0.03478543 0.03414312 0.03117578\n",
      "  0.03188839 0.03236425 0.03600036 0.03499396 0.03442287 0.03249971\n",
      "  0.03301964 0.03094166 0.02931957 0.03368161 0.03119897 0.03164203\n",
      "  0.03264634 0.03378311 0.0364469  0.03243415 0.03734117 0.03438313]\n",
      " [0.0343351  0.03201384 0.03235189 0.03263297 0.032252   0.03499038\n",
      "  0.03362629 0.03278166 0.03213532 0.03499614 0.03618791 0.03588022\n",
      "  0.03580136 0.03248975 0.03271751 0.03518241 0.03283073 0.03307272\n",
      "  0.03473946 0.0324238  0.03057594 0.03600064 0.03137271 0.03221044\n",
      "  0.03335131 0.03177955 0.03315496 0.0327578  0.0321008  0.0332544 ]\n",
      " [0.03247084 0.03091125 0.03345581 0.03310533 0.03149924 0.03409678\n",
      "  0.0337471  0.03428464 0.03306356 0.03127951 0.03290135 0.03419833\n",
      "  0.03446408 0.03434275 0.03368076 0.03553997 0.03415429 0.03144359\n",
      "  0.03422784 0.03160483 0.03366898 0.03293034 0.031681   0.03186668\n",
      "  0.03294176 0.03396305 0.03332486 0.03419073 0.03567219 0.03528859]\n",
      " [0.03321591 0.03192726 0.0314341  0.03271411 0.03220229 0.0350796\n",
      "  0.03286313 0.03146386 0.03528307 0.03439722 0.03298309 0.03378507\n",
      "  0.03415842 0.03496331 0.03332464 0.03486932 0.034747   0.03209334\n",
      "  0.03517244 0.03129071 0.03172304 0.03351249 0.03263002 0.03150803\n",
      "  0.03388385 0.03480257 0.03241563 0.03310184 0.03404548 0.03440922]\n",
      " [0.0327081  0.03342035 0.03279086 0.03190199 0.0339484  0.03352974\n",
      "  0.03264182 0.0321928  0.03293429 0.03367436 0.03314416 0.03518093\n",
      "  0.03428614 0.03475877 0.0339177  0.0350173  0.03455898 0.03262881\n",
      "  0.03362706 0.0321724  0.03361648 0.03180978 0.0309239  0.03239108\n",
      "  0.03110057 0.03517422 0.03313569 0.03465136 0.03482112 0.03334088]\n",
      " [0.03199928 0.0322732  0.03298834 0.03242284 0.03223481 0.03509196\n",
      "  0.03457413 0.03252834 0.03347673 0.03482945 0.03400245 0.03519985\n",
      "  0.0331072  0.03324499 0.03317089 0.0335583  0.03322257 0.03318843\n",
      "  0.03380566 0.03295048 0.03243049 0.03461135 0.03180956 0.0320213\n",
      "  0.03339239 0.03287475 0.03402269 0.0328442  0.03434841 0.03377498]\n",
      " [0.03344041 0.03305149 0.03327682 0.03250878 0.03383211 0.0335957\n",
      "  0.03288732 0.03296248 0.03209314 0.0338089  0.03458465 0.03349469\n",
      "  0.03481976 0.03470701 0.03211388 0.0341259  0.03491181 0.03239034\n",
      "  0.03483765 0.03275437 0.03234006 0.03259049 0.03330028 0.03334051\n",
      "  0.03236916 0.03334313 0.03310284 0.03236989 0.03259625 0.03444999]\n",
      " [0.03001891 0.03249942 0.03311313 0.03319223 0.03383134 0.03452814\n",
      "  0.03427904 0.0337039  0.03476528 0.03310929 0.03249462 0.03399123\n",
      "  0.0324748  0.03494512 0.0360707  0.03331039 0.03351876 0.03203907\n",
      "  0.03396159 0.03109697 0.03070426 0.03262947 0.03090602 0.0315985\n",
      "  0.032515   0.03448359 0.03516969 0.03311016 0.03596312 0.03597625]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 3.4045374, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/sneha/personal_stuff/dl_begin/2019-01-27_23-32-10/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.4045374.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f5a67b21860>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train one step and display the probabilties\n",
    "animal_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate done\n"
     ]
    }
   ],
   "source": [
    "results = animal_classifier.predict(\n",
    "    input_fn=test_input_fn,\n",
    "    predict_keys=None,\n",
    "    hooks=None,\n",
    "    checkpoint_path=None,\n",
    "    yield_single_examples=True)\n",
    "print('evaluate done')\n",
    "#df = pd.DataFrame(np.zeros([1,30]))\n",
    "appended_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/sneha/personal_stuff/dl_begin/2019-01-27_23-32-10/model.ckpt-1\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-c0e5fa61126e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m#print('result: {}'.format(result['probabilities']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#df.append(df2,ignore_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mappended_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    553\u001b[0m             hooks=all_hooks) as mon_sess:\n\u001b[1;32m    554\u001b[0m           \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mpreds_evaluated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0myield_single_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0;32myield\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    581\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1057\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1060\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 886\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    887\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1109\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1110\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1281\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1282\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1285\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1272\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    #print('result: {}'.format(result['probabilities']))\n",
    "    df2 = pd.DataFrame([result['probabilities']])\n",
    "    #df.append(df2,ignore_index=True)\n",
    "    appended_data.append(df2)\n",
    "appended_data = pd.concat(appended_data, axis=0)\n",
    "# write DataFrame to an excel sheet \n",
    "appended_data.to_excel('appended.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = pd.concat(appended_data, axis=0)\n",
    "# write DataFrame to an excel sheet \n",
    "appended_data.to_excel('appended.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.032823</td>\n",
       "      <td>0.033145</td>\n",
       "      <td>0.033233</td>\n",
       "      <td>0.033209</td>\n",
       "      <td>0.033913</td>\n",
       "      <td>0.033254</td>\n",
       "      <td>0.033426</td>\n",
       "      <td>0.033702</td>\n",
       "      <td>0.032676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032799</td>\n",
       "      <td>0.033343</td>\n",
       "      <td>0.032273</td>\n",
       "      <td>0.032708</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.033168</td>\n",
       "      <td>0.033620</td>\n",
       "      <td>0.033144</td>\n",
       "      <td>0.033960</td>\n",
       "      <td>0.033615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033013</td>\n",
       "      <td>0.032734</td>\n",
       "      <td>0.033213</td>\n",
       "      <td>0.033315</td>\n",
       "      <td>0.033145</td>\n",
       "      <td>0.033970</td>\n",
       "      <td>0.033404</td>\n",
       "      <td>0.033361</td>\n",
       "      <td>0.033364</td>\n",
       "      <td>0.032927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032467</td>\n",
       "      <td>0.033281</td>\n",
       "      <td>0.032655</td>\n",
       "      <td>0.032496</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>0.033744</td>\n",
       "      <td>0.033812</td>\n",
       "      <td>0.033280</td>\n",
       "      <td>0.034002</td>\n",
       "      <td>0.033667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032907</td>\n",
       "      <td>0.033005</td>\n",
       "      <td>0.033122</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.033145</td>\n",
       "      <td>0.033925</td>\n",
       "      <td>0.033033</td>\n",
       "      <td>0.033193</td>\n",
       "      <td>0.033633</td>\n",
       "      <td>0.032826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032758</td>\n",
       "      <td>0.033479</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>0.032654</td>\n",
       "      <td>0.032883</td>\n",
       "      <td>0.033431</td>\n",
       "      <td>0.033545</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.034032</td>\n",
       "      <td>0.033689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032985</td>\n",
       "      <td>0.032598</td>\n",
       "      <td>0.033162</td>\n",
       "      <td>0.033320</td>\n",
       "      <td>0.033124</td>\n",
       "      <td>0.033880</td>\n",
       "      <td>0.033139</td>\n",
       "      <td>0.033619</td>\n",
       "      <td>0.033772</td>\n",
       "      <td>0.032825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032703</td>\n",
       "      <td>0.033665</td>\n",
       "      <td>0.032736</td>\n",
       "      <td>0.032393</td>\n",
       "      <td>0.032971</td>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.033707</td>\n",
       "      <td>0.033168</td>\n",
       "      <td>0.033798</td>\n",
       "      <td>0.033470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032996</td>\n",
       "      <td>0.032830</td>\n",
       "      <td>0.033143</td>\n",
       "      <td>0.033293</td>\n",
       "      <td>0.033244</td>\n",
       "      <td>0.033795</td>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.033338</td>\n",
       "      <td>0.033579</td>\n",
       "      <td>0.032848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032793</td>\n",
       "      <td>0.033412</td>\n",
       "      <td>0.032796</td>\n",
       "      <td>0.032894</td>\n",
       "      <td>0.032852</td>\n",
       "      <td>0.033465</td>\n",
       "      <td>0.033699</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.034034</td>\n",
       "      <td>0.033587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032928</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>0.032983</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>0.033221</td>\n",
       "      <td>0.034060</td>\n",
       "      <td>0.033322</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.033675</td>\n",
       "      <td>0.033070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032488</td>\n",
       "      <td>0.032974</td>\n",
       "      <td>0.032532</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>0.032978</td>\n",
       "      <td>0.033566</td>\n",
       "      <td>0.033556</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.034136</td>\n",
       "      <td>0.033647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032964</td>\n",
       "      <td>0.032489</td>\n",
       "      <td>0.032678</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.033234</td>\n",
       "      <td>0.034558</td>\n",
       "      <td>0.033250</td>\n",
       "      <td>0.032991</td>\n",
       "      <td>0.033736</td>\n",
       "      <td>0.032875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032320</td>\n",
       "      <td>0.033269</td>\n",
       "      <td>0.032247</td>\n",
       "      <td>0.032545</td>\n",
       "      <td>0.032726</td>\n",
       "      <td>0.033869</td>\n",
       "      <td>0.033384</td>\n",
       "      <td>0.033059</td>\n",
       "      <td>0.034323</td>\n",
       "      <td>0.033889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033086</td>\n",
       "      <td>0.032923</td>\n",
       "      <td>0.033194</td>\n",
       "      <td>0.033511</td>\n",
       "      <td>0.032937</td>\n",
       "      <td>0.033788</td>\n",
       "      <td>0.033486</td>\n",
       "      <td>0.033513</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>0.033024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032689</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.032815</td>\n",
       "      <td>0.032945</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>0.033794</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>0.033424</td>\n",
       "      <td>0.033861</td>\n",
       "      <td>0.033353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033161</td>\n",
       "      <td>0.032528</td>\n",
       "      <td>0.033177</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.032914</td>\n",
       "      <td>0.033946</td>\n",
       "      <td>0.033987</td>\n",
       "      <td>0.033284</td>\n",
       "      <td>0.033322</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.033327</td>\n",
       "      <td>0.032658</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.033513</td>\n",
       "      <td>0.033924</td>\n",
       "      <td>0.033613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033061</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>0.033143</td>\n",
       "      <td>0.033170</td>\n",
       "      <td>0.033299</td>\n",
       "      <td>0.033925</td>\n",
       "      <td>0.033158</td>\n",
       "      <td>0.032985</td>\n",
       "      <td>0.033841</td>\n",
       "      <td>0.032852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032856</td>\n",
       "      <td>0.033836</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.032441</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.033619</td>\n",
       "      <td>0.033298</td>\n",
       "      <td>0.033935</td>\n",
       "      <td>0.033341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033134</td>\n",
       "      <td>0.032830</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>0.033302</td>\n",
       "      <td>0.033795</td>\n",
       "      <td>0.033302</td>\n",
       "      <td>0.033631</td>\n",
       "      <td>0.033374</td>\n",
       "      <td>0.032909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032580</td>\n",
       "      <td>0.033069</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.032518</td>\n",
       "      <td>0.032684</td>\n",
       "      <td>0.033525</td>\n",
       "      <td>0.033554</td>\n",
       "      <td>0.033213</td>\n",
       "      <td>0.034342</td>\n",
       "      <td>0.033784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032628</td>\n",
       "      <td>0.032821</td>\n",
       "      <td>0.033185</td>\n",
       "      <td>0.033234</td>\n",
       "      <td>0.033068</td>\n",
       "      <td>0.033883</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>0.032740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.033645</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>0.032726</td>\n",
       "      <td>0.033941</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>0.034041</td>\n",
       "      <td>0.033713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032923</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.033130</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.033217</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>0.033108</td>\n",
       "      <td>0.033326</td>\n",
       "      <td>0.033614</td>\n",
       "      <td>0.032778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032649</td>\n",
       "      <td>0.033395</td>\n",
       "      <td>0.032567</td>\n",
       "      <td>0.032640</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>0.033365</td>\n",
       "      <td>0.033293</td>\n",
       "      <td>0.033969</td>\n",
       "      <td>0.033738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.033045</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>0.033304</td>\n",
       "      <td>0.033906</td>\n",
       "      <td>0.033249</td>\n",
       "      <td>0.033364</td>\n",
       "      <td>0.033468</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032812</td>\n",
       "      <td>0.033317</td>\n",
       "      <td>0.032587</td>\n",
       "      <td>0.032647</td>\n",
       "      <td>0.032696</td>\n",
       "      <td>0.033487</td>\n",
       "      <td>0.033506</td>\n",
       "      <td>0.033277</td>\n",
       "      <td>0.034052</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033226</td>\n",
       "      <td>0.032840</td>\n",
       "      <td>0.033187</td>\n",
       "      <td>0.032996</td>\n",
       "      <td>0.033283</td>\n",
       "      <td>0.034136</td>\n",
       "      <td>0.033389</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.033536</td>\n",
       "      <td>0.033103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032634</td>\n",
       "      <td>0.033184</td>\n",
       "      <td>0.032408</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.032884</td>\n",
       "      <td>0.033265</td>\n",
       "      <td>0.033711</td>\n",
       "      <td>0.033134</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.033765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033145</td>\n",
       "      <td>0.033193</td>\n",
       "      <td>0.033284</td>\n",
       "      <td>0.033330</td>\n",
       "      <td>0.033290</td>\n",
       "      <td>0.033901</td>\n",
       "      <td>0.033239</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>0.033656</td>\n",
       "      <td>0.032807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032603</td>\n",
       "      <td>0.033226</td>\n",
       "      <td>0.032559</td>\n",
       "      <td>0.032669</td>\n",
       "      <td>0.032879</td>\n",
       "      <td>0.033368</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.033143</td>\n",
       "      <td>0.033994</td>\n",
       "      <td>0.033849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033252</td>\n",
       "      <td>0.032928</td>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.033243</td>\n",
       "      <td>0.033212</td>\n",
       "      <td>0.034057</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>0.033722</td>\n",
       "      <td>0.032878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032434</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.032284</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.032678</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.033474</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.034229</td>\n",
       "      <td>0.033755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033266</td>\n",
       "      <td>0.033146</td>\n",
       "      <td>0.032836</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033914</td>\n",
       "      <td>0.033325</td>\n",
       "      <td>0.033172</td>\n",
       "      <td>0.033576</td>\n",
       "      <td>0.033205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032740</td>\n",
       "      <td>0.033184</td>\n",
       "      <td>0.032493</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>0.033067</td>\n",
       "      <td>0.033350</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.033141</td>\n",
       "      <td>0.033811</td>\n",
       "      <td>0.033692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033105</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.033449</td>\n",
       "      <td>0.033214</td>\n",
       "      <td>0.033399</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>0.033428</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.033531</td>\n",
       "      <td>0.033075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032524</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.032614</td>\n",
       "      <td>0.032664</td>\n",
       "      <td>0.032680</td>\n",
       "      <td>0.033595</td>\n",
       "      <td>0.033738</td>\n",
       "      <td>0.033281</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.033491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032991</td>\n",
       "      <td>0.032664</td>\n",
       "      <td>0.033035</td>\n",
       "      <td>0.033413</td>\n",
       "      <td>0.033094</td>\n",
       "      <td>0.033845</td>\n",
       "      <td>0.033357</td>\n",
       "      <td>0.033484</td>\n",
       "      <td>0.033390</td>\n",
       "      <td>0.032975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032580</td>\n",
       "      <td>0.033222</td>\n",
       "      <td>0.032656</td>\n",
       "      <td>0.032510</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.033532</td>\n",
       "      <td>0.033622</td>\n",
       "      <td>0.033162</td>\n",
       "      <td>0.034032</td>\n",
       "      <td>0.033724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033310</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>0.033141</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.033355</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>0.033436</td>\n",
       "      <td>0.033403</td>\n",
       "      <td>0.033556</td>\n",
       "      <td>0.033161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033118</td>\n",
       "      <td>0.033524</td>\n",
       "      <td>0.032741</td>\n",
       "      <td>0.033035</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.033087</td>\n",
       "      <td>0.033367</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>0.033693</td>\n",
       "      <td>0.033521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033101</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.033177</td>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.033204</td>\n",
       "      <td>0.034050</td>\n",
       "      <td>0.033431</td>\n",
       "      <td>0.033177</td>\n",
       "      <td>0.033539</td>\n",
       "      <td>0.032751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032544</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.032283</td>\n",
       "      <td>0.032467</td>\n",
       "      <td>0.032892</td>\n",
       "      <td>0.033458</td>\n",
       "      <td>0.033627</td>\n",
       "      <td>0.033140</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>0.033789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032485</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>0.033146</td>\n",
       "      <td>0.033067</td>\n",
       "      <td>0.033176</td>\n",
       "      <td>0.034099</td>\n",
       "      <td>0.033116</td>\n",
       "      <td>0.033209</td>\n",
       "      <td>0.033540</td>\n",
       "      <td>0.032970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032775</td>\n",
       "      <td>0.034182</td>\n",
       "      <td>0.032603</td>\n",
       "      <td>0.033177</td>\n",
       "      <td>0.032548</td>\n",
       "      <td>0.033457</td>\n",
       "      <td>0.033623</td>\n",
       "      <td>0.033291</td>\n",
       "      <td>0.034120</td>\n",
       "      <td>0.033454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032802</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.033220</td>\n",
       "      <td>0.032973</td>\n",
       "      <td>0.033148</td>\n",
       "      <td>0.033798</td>\n",
       "      <td>0.033512</td>\n",
       "      <td>0.033282</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.033088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.033677</td>\n",
       "      <td>0.032920</td>\n",
       "      <td>0.033015</td>\n",
       "      <td>0.032898</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>0.033453</td>\n",
       "      <td>0.033390</td>\n",
       "      <td>0.033761</td>\n",
       "      <td>0.033248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032858</td>\n",
       "      <td>0.033145</td>\n",
       "      <td>0.033019</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.033957</td>\n",
       "      <td>0.033077</td>\n",
       "      <td>0.033045</td>\n",
       "      <td>0.033669</td>\n",
       "      <td>0.032604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.033314</td>\n",
       "      <td>0.032640</td>\n",
       "      <td>0.032444</td>\n",
       "      <td>0.032836</td>\n",
       "      <td>0.033749</td>\n",
       "      <td>0.033321</td>\n",
       "      <td>0.033233</td>\n",
       "      <td>0.033941</td>\n",
       "      <td>0.033605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.033171</td>\n",
       "      <td>0.033132</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.033160</td>\n",
       "      <td>0.033958</td>\n",
       "      <td>0.033154</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>0.033707</td>\n",
       "      <td>0.033008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.033572</td>\n",
       "      <td>0.032881</td>\n",
       "      <td>0.032757</td>\n",
       "      <td>0.032651</td>\n",
       "      <td>0.033248</td>\n",
       "      <td>0.033496</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.033801</td>\n",
       "      <td>0.033395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033135</td>\n",
       "      <td>0.033105</td>\n",
       "      <td>0.033171</td>\n",
       "      <td>0.033234</td>\n",
       "      <td>0.033221</td>\n",
       "      <td>0.033650</td>\n",
       "      <td>0.033209</td>\n",
       "      <td>0.033157</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>0.033045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033139</td>\n",
       "      <td>0.033461</td>\n",
       "      <td>0.032764</td>\n",
       "      <td>0.033007</td>\n",
       "      <td>0.032929</td>\n",
       "      <td>0.033440</td>\n",
       "      <td>0.033554</td>\n",
       "      <td>0.033192</td>\n",
       "      <td>0.033659</td>\n",
       "      <td>0.033563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032915</td>\n",
       "      <td>0.032906</td>\n",
       "      <td>0.032674</td>\n",
       "      <td>0.032993</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.034178</td>\n",
       "      <td>0.033296</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032588</td>\n",
       "      <td>0.033658</td>\n",
       "      <td>0.032211</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.032883</td>\n",
       "      <td>0.033527</td>\n",
       "      <td>0.033578</td>\n",
       "      <td>0.033191</td>\n",
       "      <td>0.034144</td>\n",
       "      <td>0.033602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.033381</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.033289</td>\n",
       "      <td>0.033229</td>\n",
       "      <td>0.033555</td>\n",
       "      <td>0.033187</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>0.033714</td>\n",
       "      <td>0.033002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.033401</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.032947</td>\n",
       "      <td>0.032901</td>\n",
       "      <td>0.033496</td>\n",
       "      <td>0.033205</td>\n",
       "      <td>0.033528</td>\n",
       "      <td>0.033619</td>\n",
       "      <td>0.033586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032851</td>\n",
       "      <td>0.033091</td>\n",
       "      <td>0.033347</td>\n",
       "      <td>0.033091</td>\n",
       "      <td>0.033221</td>\n",
       "      <td>0.033801</td>\n",
       "      <td>0.033109</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>0.033630</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032987</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.032790</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.033566</td>\n",
       "      <td>0.033582</td>\n",
       "      <td>0.033544</td>\n",
       "      <td>0.034012</td>\n",
       "      <td>0.033252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032884</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>0.033106</td>\n",
       "      <td>0.033601</td>\n",
       "      <td>0.033283</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>0.033219</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>0.033261</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032539</td>\n",
       "      <td>0.033435</td>\n",
       "      <td>0.032753</td>\n",
       "      <td>0.032688</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.033602</td>\n",
       "      <td>0.033631</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.033967</td>\n",
       "      <td>0.033687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032874</td>\n",
       "      <td>0.032866</td>\n",
       "      <td>0.033156</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>0.033551</td>\n",
       "      <td>0.033882</td>\n",
       "      <td>0.033115</td>\n",
       "      <td>0.033101</td>\n",
       "      <td>0.033547</td>\n",
       "      <td>0.032772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032713</td>\n",
       "      <td>0.033363</td>\n",
       "      <td>0.032608</td>\n",
       "      <td>0.032665</td>\n",
       "      <td>0.032758</td>\n",
       "      <td>0.033696</td>\n",
       "      <td>0.033388</td>\n",
       "      <td>0.033179</td>\n",
       "      <td>0.033827</td>\n",
       "      <td>0.033746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032771</td>\n",
       "      <td>0.033539</td>\n",
       "      <td>0.033395</td>\n",
       "      <td>0.033054</td>\n",
       "      <td>0.034173</td>\n",
       "      <td>0.033423</td>\n",
       "      <td>0.033509</td>\n",
       "      <td>0.033665</td>\n",
       "      <td>0.033097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032465</td>\n",
       "      <td>0.033673</td>\n",
       "      <td>0.032848</td>\n",
       "      <td>0.032472</td>\n",
       "      <td>0.032929</td>\n",
       "      <td>0.033184</td>\n",
       "      <td>0.033527</td>\n",
       "      <td>0.033327</td>\n",
       "      <td>0.033991</td>\n",
       "      <td>0.033276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032863</td>\n",
       "      <td>0.032481</td>\n",
       "      <td>0.032890</td>\n",
       "      <td>0.033309</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.033976</td>\n",
       "      <td>0.033547</td>\n",
       "      <td>0.033632</td>\n",
       "      <td>0.033770</td>\n",
       "      <td>0.032980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032529</td>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.032112</td>\n",
       "      <td>0.032245</td>\n",
       "      <td>0.032663</td>\n",
       "      <td>0.033626</td>\n",
       "      <td>0.033655</td>\n",
       "      <td>0.033299</td>\n",
       "      <td>0.034367</td>\n",
       "      <td>0.034055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033085</td>\n",
       "      <td>0.033214</td>\n",
       "      <td>0.033043</td>\n",
       "      <td>0.033145</td>\n",
       "      <td>0.033236</td>\n",
       "      <td>0.033858</td>\n",
       "      <td>0.033255</td>\n",
       "      <td>0.033033</td>\n",
       "      <td>0.033494</td>\n",
       "      <td>0.032991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032815</td>\n",
       "      <td>0.033307</td>\n",
       "      <td>0.032695</td>\n",
       "      <td>0.032878</td>\n",
       "      <td>0.033075</td>\n",
       "      <td>0.033651</td>\n",
       "      <td>0.033290</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>0.033639</td>\n",
       "      <td>0.033348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033107</td>\n",
       "      <td>0.033039</td>\n",
       "      <td>0.033259</td>\n",
       "      <td>0.033135</td>\n",
       "      <td>0.033239</td>\n",
       "      <td>0.033644</td>\n",
       "      <td>0.033375</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.033131</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032653</td>\n",
       "      <td>0.032930</td>\n",
       "      <td>0.032660</td>\n",
       "      <td>0.032676</td>\n",
       "      <td>0.033059</td>\n",
       "      <td>0.033930</td>\n",
       "      <td>0.033698</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>0.033942</td>\n",
       "      <td>0.033739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033122</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.032808</td>\n",
       "      <td>0.033376</td>\n",
       "      <td>0.033168</td>\n",
       "      <td>0.033774</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.033210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032792</td>\n",
       "      <td>0.032839</td>\n",
       "      <td>0.032431</td>\n",
       "      <td>0.032554</td>\n",
       "      <td>0.033044</td>\n",
       "      <td>0.033674</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.033247</td>\n",
       "      <td>0.033978</td>\n",
       "      <td>0.033905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033131</td>\n",
       "      <td>0.033090</td>\n",
       "      <td>0.033159</td>\n",
       "      <td>0.033429</td>\n",
       "      <td>0.033179</td>\n",
       "      <td>0.033660</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.033551</td>\n",
       "      <td>0.033105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>0.032843</td>\n",
       "      <td>0.032809</td>\n",
       "      <td>0.033009</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.033381</td>\n",
       "      <td>0.033226</td>\n",
       "      <td>0.033824</td>\n",
       "      <td>0.033537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033090</td>\n",
       "      <td>0.032708</td>\n",
       "      <td>0.032849</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.032712</td>\n",
       "      <td>0.033567</td>\n",
       "      <td>0.033603</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.033510</td>\n",
       "      <td>0.033098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032590</td>\n",
       "      <td>0.033049</td>\n",
       "      <td>0.032562</td>\n",
       "      <td>0.032904</td>\n",
       "      <td>0.033023</td>\n",
       "      <td>0.033686</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>0.033664</td>\n",
       "      <td>0.033649</td>\n",
       "      <td>0.033981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032994</td>\n",
       "      <td>0.033011</td>\n",
       "      <td>0.033198</td>\n",
       "      <td>0.033255</td>\n",
       "      <td>0.033302</td>\n",
       "      <td>0.033882</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.033084</td>\n",
       "      <td>0.033515</td>\n",
       "      <td>0.032796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032871</td>\n",
       "      <td>0.032957</td>\n",
       "      <td>0.032588</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>0.033541</td>\n",
       "      <td>0.033726</td>\n",
       "      <td>0.033266</td>\n",
       "      <td>0.034224</td>\n",
       "      <td>0.033815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032624</td>\n",
       "      <td>0.032786</td>\n",
       "      <td>0.032716</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>0.033179</td>\n",
       "      <td>0.034193</td>\n",
       "      <td>0.033240</td>\n",
       "      <td>0.032749</td>\n",
       "      <td>0.033823</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>0.033687</td>\n",
       "      <td>0.032435</td>\n",
       "      <td>0.032446</td>\n",
       "      <td>0.032807</td>\n",
       "      <td>0.033670</td>\n",
       "      <td>0.033542</td>\n",
       "      <td>0.033316</td>\n",
       "      <td>0.034195</td>\n",
       "      <td>0.033748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032858</td>\n",
       "      <td>0.032915</td>\n",
       "      <td>0.033193</td>\n",
       "      <td>0.033349</td>\n",
       "      <td>0.033062</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>0.033307</td>\n",
       "      <td>0.033401</td>\n",
       "      <td>0.033579</td>\n",
       "      <td>0.032839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032558</td>\n",
       "      <td>0.033323</td>\n",
       "      <td>0.032517</td>\n",
       "      <td>0.032651</td>\n",
       "      <td>0.032920</td>\n",
       "      <td>0.033450</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>0.033437</td>\n",
       "      <td>0.033938</td>\n",
       "      <td>0.033569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.033112</td>\n",
       "      <td>0.033124</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>0.033763</td>\n",
       "      <td>0.033376</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>0.033485</td>\n",
       "      <td>0.033053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032733</td>\n",
       "      <td>0.033331</td>\n",
       "      <td>0.032632</td>\n",
       "      <td>0.032801</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>0.033876</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>0.033739</td>\n",
       "      <td>0.033233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033007</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.033392</td>\n",
       "      <td>0.033248</td>\n",
       "      <td>0.033133</td>\n",
       "      <td>0.033906</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>0.033204</td>\n",
       "      <td>0.033686</td>\n",
       "      <td>0.032866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>0.033426</td>\n",
       "      <td>0.032689</td>\n",
       "      <td>0.032717</td>\n",
       "      <td>0.032950</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.033634</td>\n",
       "      <td>0.033381</td>\n",
       "      <td>0.034018</td>\n",
       "      <td>0.033737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033148</td>\n",
       "      <td>0.032886</td>\n",
       "      <td>0.033116</td>\n",
       "      <td>0.033191</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>0.034148</td>\n",
       "      <td>0.033115</td>\n",
       "      <td>0.033453</td>\n",
       "      <td>0.034006</td>\n",
       "      <td>0.033197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.033586</td>\n",
       "      <td>0.032392</td>\n",
       "      <td>0.032331</td>\n",
       "      <td>0.032764</td>\n",
       "      <td>0.033303</td>\n",
       "      <td>0.033569</td>\n",
       "      <td>0.033429</td>\n",
       "      <td>0.033912</td>\n",
       "      <td>0.033405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032792</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>0.033073</td>\n",
       "      <td>0.033153</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.033336</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.032933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032688</td>\n",
       "      <td>0.033401</td>\n",
       "      <td>0.032576</td>\n",
       "      <td>0.032909</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.033578</td>\n",
       "      <td>0.033232</td>\n",
       "      <td>0.033756</td>\n",
       "      <td>0.033620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.033211</td>\n",
       "      <td>0.033330</td>\n",
       "      <td>0.034027</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.033118</td>\n",
       "      <td>0.033619</td>\n",
       "      <td>0.032858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.033343</td>\n",
       "      <td>0.032724</td>\n",
       "      <td>0.032445</td>\n",
       "      <td>0.032822</td>\n",
       "      <td>0.033537</td>\n",
       "      <td>0.033538</td>\n",
       "      <td>0.033099</td>\n",
       "      <td>0.034132</td>\n",
       "      <td>0.033836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032809</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>0.033225</td>\n",
       "      <td>0.033417</td>\n",
       "      <td>0.033159</td>\n",
       "      <td>0.034049</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.033399</td>\n",
       "      <td>0.033583</td>\n",
       "      <td>0.033082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032534</td>\n",
       "      <td>0.033290</td>\n",
       "      <td>0.032474</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.032950</td>\n",
       "      <td>0.033280</td>\n",
       "      <td>0.033771</td>\n",
       "      <td>0.033611</td>\n",
       "      <td>0.033986</td>\n",
       "      <td>0.033659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032873</td>\n",
       "      <td>0.032983</td>\n",
       "      <td>0.033190</td>\n",
       "      <td>0.033277</td>\n",
       "      <td>0.033348</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.033297</td>\n",
       "      <td>0.033255</td>\n",
       "      <td>0.033729</td>\n",
       "      <td>0.033011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032894</td>\n",
       "      <td>0.033428</td>\n",
       "      <td>0.032683</td>\n",
       "      <td>0.033018</td>\n",
       "      <td>0.032988</td>\n",
       "      <td>0.033374</td>\n",
       "      <td>0.033271</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.033766</td>\n",
       "      <td>0.033551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033095</td>\n",
       "      <td>0.032374</td>\n",
       "      <td>0.033240</td>\n",
       "      <td>0.033484</td>\n",
       "      <td>0.033134</td>\n",
       "      <td>0.034037</td>\n",
       "      <td>0.033540</td>\n",
       "      <td>0.033628</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>0.032857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032257</td>\n",
       "      <td>0.033275</td>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.032393</td>\n",
       "      <td>0.032773</td>\n",
       "      <td>0.033484</td>\n",
       "      <td>0.033914</td>\n",
       "      <td>0.033176</td>\n",
       "      <td>0.034167</td>\n",
       "      <td>0.033929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033177</td>\n",
       "      <td>0.032606</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>0.033267</td>\n",
       "      <td>0.033116</td>\n",
       "      <td>0.034067</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>0.033722</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.032832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032464</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.032345</td>\n",
       "      <td>0.032546</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.033338</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.033954</td>\n",
       "      <td>0.033968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033043</td>\n",
       "      <td>0.032866</td>\n",
       "      <td>0.033389</td>\n",
       "      <td>0.033396</td>\n",
       "      <td>0.033017</td>\n",
       "      <td>0.033899</td>\n",
       "      <td>0.033311</td>\n",
       "      <td>0.033305</td>\n",
       "      <td>0.033702</td>\n",
       "      <td>0.032689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032632</td>\n",
       "      <td>0.033506</td>\n",
       "      <td>0.032731</td>\n",
       "      <td>0.032626</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.033188</td>\n",
       "      <td>0.033594</td>\n",
       "      <td>0.033207</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.033609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033003</td>\n",
       "      <td>0.032952</td>\n",
       "      <td>0.033282</td>\n",
       "      <td>0.033184</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>0.033875</td>\n",
       "      <td>0.033324</td>\n",
       "      <td>0.033125</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.032644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.033162</td>\n",
       "      <td>0.032604</td>\n",
       "      <td>0.032811</td>\n",
       "      <td>0.032588</td>\n",
       "      <td>0.033530</td>\n",
       "      <td>0.033462</td>\n",
       "      <td>0.033425</td>\n",
       "      <td>0.033970</td>\n",
       "      <td>0.033718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033053</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.033184</td>\n",
       "      <td>0.033396</td>\n",
       "      <td>0.033405</td>\n",
       "      <td>0.033748</td>\n",
       "      <td>0.033248</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.033674</td>\n",
       "      <td>0.032916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032754</td>\n",
       "      <td>0.033207</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>0.032607</td>\n",
       "      <td>0.032850</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.033405</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.033889</td>\n",
       "      <td>0.033527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032935</td>\n",
       "      <td>0.032849</td>\n",
       "      <td>0.032868</td>\n",
       "      <td>0.032940</td>\n",
       "      <td>0.033014</td>\n",
       "      <td>0.033843</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.032959</td>\n",
       "      <td>0.033569</td>\n",
       "      <td>0.032994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033008</td>\n",
       "      <td>0.033090</td>\n",
       "      <td>0.032379</td>\n",
       "      <td>0.032323</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.034088</td>\n",
       "      <td>0.033799</td>\n",
       "      <td>0.033131</td>\n",
       "      <td>0.034290</td>\n",
       "      <td>0.033588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032843</td>\n",
       "      <td>0.033439</td>\n",
       "      <td>0.033421</td>\n",
       "      <td>0.033222</td>\n",
       "      <td>0.033067</td>\n",
       "      <td>0.034104</td>\n",
       "      <td>0.032932</td>\n",
       "      <td>0.033016</td>\n",
       "      <td>0.034017</td>\n",
       "      <td>0.032490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032262</td>\n",
       "      <td>0.033435</td>\n",
       "      <td>0.032358</td>\n",
       "      <td>0.032240</td>\n",
       "      <td>0.032577</td>\n",
       "      <td>0.033760</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>0.033366</td>\n",
       "      <td>0.034235</td>\n",
       "      <td>0.033918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033198</td>\n",
       "      <td>0.033292</td>\n",
       "      <td>0.033296</td>\n",
       "      <td>0.033449</td>\n",
       "      <td>0.033148</td>\n",
       "      <td>0.033753</td>\n",
       "      <td>0.032940</td>\n",
       "      <td>0.033324</td>\n",
       "      <td>0.033375</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>0.033754</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>0.032772</td>\n",
       "      <td>0.032868</td>\n",
       "      <td>0.033138</td>\n",
       "      <td>0.033360</td>\n",
       "      <td>0.033367</td>\n",
       "      <td>0.033950</td>\n",
       "      <td>0.033557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.032757</td>\n",
       "      <td>0.033186</td>\n",
       "      <td>0.033625</td>\n",
       "      <td>0.033103</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>0.033559</td>\n",
       "      <td>0.033543</td>\n",
       "      <td>0.033861</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032509</td>\n",
       "      <td>0.033245</td>\n",
       "      <td>0.032642</td>\n",
       "      <td>0.032481</td>\n",
       "      <td>0.032862</td>\n",
       "      <td>0.033624</td>\n",
       "      <td>0.033438</td>\n",
       "      <td>0.033046</td>\n",
       "      <td>0.034178</td>\n",
       "      <td>0.033542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032915</td>\n",
       "      <td>0.033068</td>\n",
       "      <td>0.033088</td>\n",
       "      <td>0.033508</td>\n",
       "      <td>0.033006</td>\n",
       "      <td>0.033725</td>\n",
       "      <td>0.033303</td>\n",
       "      <td>0.033190</td>\n",
       "      <td>0.033550</td>\n",
       "      <td>0.032702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032721</td>\n",
       "      <td>0.033457</td>\n",
       "      <td>0.032664</td>\n",
       "      <td>0.032622</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.033651</td>\n",
       "      <td>0.033515</td>\n",
       "      <td>0.033170</td>\n",
       "      <td>0.034242</td>\n",
       "      <td>0.033794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>0.033004</td>\n",
       "      <td>0.033154</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>0.033869</td>\n",
       "      <td>0.033252</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>0.033151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032649</td>\n",
       "      <td>0.033506</td>\n",
       "      <td>0.032836</td>\n",
       "      <td>0.032947</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.033407</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.033150</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.033483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.033262  0.032823  0.033145  0.033233  0.033209  0.033913  0.033254   \n",
       "0   0.033013  0.032734  0.033213  0.033315  0.033145  0.033970  0.033404   \n",
       "0   0.032907  0.033005  0.033122  0.033301  0.033145  0.033925  0.033033   \n",
       "0   0.032985  0.032598  0.033162  0.033320  0.033124  0.033880  0.033139   \n",
       "0   0.032996  0.032830  0.033143  0.033293  0.033244  0.033795  0.033262   \n",
       "0   0.032928  0.032766  0.032983  0.033501  0.033221  0.034060  0.033322   \n",
       "0   0.032964  0.032489  0.032678  0.033230  0.033234  0.034558  0.033250   \n",
       "0   0.033086  0.032923  0.033194  0.033511  0.032937  0.033788  0.033486   \n",
       "0   0.033161  0.032528  0.033177  0.033398  0.032914  0.033946  0.033987   \n",
       "0   0.033061  0.032829  0.033143  0.033170  0.033299  0.033925  0.033158   \n",
       "0   0.033134  0.032830  0.032805  0.033358  0.033302  0.033795  0.033302   \n",
       "0   0.032628  0.032821  0.033185  0.033234  0.033068  0.033883  0.033203   \n",
       "0   0.032923  0.032880  0.033130  0.033278  0.033217  0.033923  0.033108   \n",
       "0   0.032895  0.032860  0.033045  0.033354  0.033304  0.033906  0.033249   \n",
       "0   0.033226  0.032840  0.033187  0.032996  0.033283  0.034136  0.033389   \n",
       "0   0.033145  0.033193  0.033284  0.033330  0.033290  0.033901  0.033239   \n",
       "0   0.033252  0.032928  0.033012  0.033243  0.033212  0.034057  0.033246   \n",
       "0   0.033266  0.033146  0.032836  0.032990  0.033333  0.033914  0.033325   \n",
       "0   0.033105  0.032895  0.033449  0.033214  0.033399  0.034048  0.033428   \n",
       "0   0.032991  0.032664  0.033035  0.033413  0.033094  0.033845  0.033357   \n",
       "0   0.033310  0.032863  0.033141  0.033230  0.033355  0.033746  0.033436   \n",
       "0   0.033101  0.032641  0.033177  0.033274  0.033204  0.034050  0.033431   \n",
       "0   0.032485  0.032889  0.033146  0.033067  0.033176  0.034099  0.033116   \n",
       "0   0.032802  0.033268  0.033220  0.032973  0.033148  0.033798  0.033512   \n",
       "0   0.032858  0.033145  0.033019  0.033167  0.033241  0.033957  0.033077   \n",
       "0   0.032956  0.033171  0.033132  0.033230  0.033160  0.033958  0.033154   \n",
       "0   0.033135  0.033105  0.033171  0.033234  0.033221  0.033650  0.033209   \n",
       "0   0.032915  0.032906  0.032674  0.032993  0.033149  0.034178  0.033296   \n",
       "0   0.033012  0.033381  0.033113  0.033289  0.033229  0.033555  0.033187   \n",
       "0   0.032851  0.033091  0.033347  0.033091  0.033221  0.033801  0.033109   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "0   0.032884  0.032829  0.033106  0.033601  0.033283  0.033621  0.033219   \n",
       "0   0.032874  0.032866  0.033156  0.033478  0.033551  0.033882  0.033115   \n",
       "0   0.032880  0.032771  0.033539  0.033395  0.033054  0.034173  0.033423   \n",
       "0   0.032863  0.032481  0.032890  0.033309  0.033072  0.033976  0.033547   \n",
       "0   0.033085  0.033214  0.033043  0.033145  0.033236  0.033858  0.033255   \n",
       "0   0.033107  0.033039  0.033259  0.033135  0.033239  0.033644  0.033375   \n",
       "0   0.033122  0.032599  0.032808  0.033376  0.033168  0.033774  0.033678   \n",
       "0   0.033131  0.033090  0.033159  0.033429  0.033179  0.033660  0.033183   \n",
       "0   0.033090  0.032708  0.032849  0.033387  0.032712  0.033567  0.033603   \n",
       "0   0.032994  0.033011  0.033198  0.033255  0.033302  0.033882  0.033369   \n",
       "0   0.032624  0.032786  0.032716  0.033312  0.033179  0.034193  0.033240   \n",
       "0   0.032858  0.032915  0.033193  0.033349  0.033062  0.033993  0.033307   \n",
       "0   0.033274  0.033112  0.033124  0.033447  0.033231  0.033763  0.033376   \n",
       "0   0.033007  0.032943  0.033392  0.033248  0.033133  0.033906  0.033183   \n",
       "0   0.033148  0.032886  0.033116  0.033191  0.032984  0.034148  0.033115   \n",
       "0   0.032792  0.032805  0.033073  0.033153  0.033345  0.033977  0.033336   \n",
       "0   0.032992  0.032922  0.032922  0.033211  0.033330  0.034027  0.033169   \n",
       "0   0.032809  0.032616  0.033225  0.033417  0.033159  0.034049  0.033337   \n",
       "0   0.032873  0.032983  0.033190  0.033277  0.033348  0.034072  0.033297   \n",
       "0   0.033095  0.032374  0.033240  0.033484  0.033134  0.034037  0.033540   \n",
       "0   0.033177  0.032606  0.032829  0.033267  0.033116  0.034067  0.033447   \n",
       "0   0.033043  0.032866  0.033389  0.033396  0.033017  0.033899  0.033311   \n",
       "0   0.033003  0.032952  0.033282  0.033184  0.033246  0.033875  0.033324   \n",
       "0   0.033053  0.032960  0.033184  0.033396  0.033405  0.033748  0.033248   \n",
       "0   0.032935  0.032849  0.032868  0.032940  0.033014  0.033843  0.033482   \n",
       "0   0.032843  0.033439  0.033421  0.033222  0.033067  0.034104  0.032932   \n",
       "0   0.033198  0.033292  0.033296  0.033449  0.033148  0.033753  0.032940   \n",
       "0   0.033072  0.032757  0.033186  0.033625  0.033103  0.033982  0.033559   \n",
       "0   0.032915  0.033068  0.033088  0.033508  0.033006  0.033725  0.033303   \n",
       "0   0.032995  0.033166  0.033004  0.033154  0.033312  0.033869  0.033252   \n",
       "\n",
       "          7         8         9     ...           20        21        22  \\\n",
       "0   0.033426  0.033702  0.032676    ...     0.032799  0.033343  0.032273   \n",
       "0   0.033361  0.033364  0.032927    ...     0.032467  0.033281  0.032655   \n",
       "0   0.033193  0.033633  0.032826    ...     0.032758  0.033479  0.032593   \n",
       "0   0.033619  0.033772  0.032825    ...     0.032703  0.033665  0.032736   \n",
       "0   0.033338  0.033579  0.032848    ...     0.032793  0.033412  0.032796   \n",
       "0   0.033295  0.033675  0.033070    ...     0.032488  0.032974  0.032532   \n",
       "0   0.032991  0.033736  0.032875    ...     0.032320  0.033269  0.032247   \n",
       "0   0.033513  0.033488  0.033024    ...     0.032689  0.033398  0.032815   \n",
       "0   0.033284  0.033322  0.033231    ...     0.032600  0.033327  0.032658   \n",
       "0   0.032985  0.033841  0.032852    ...     0.032856  0.033836  0.032516   \n",
       "0   0.033631  0.033374  0.032909    ...     0.032580  0.033069  0.032258   \n",
       "0   0.032989  0.033445  0.032740    ...     0.032880  0.033645  0.032638   \n",
       "0   0.033326  0.033614  0.032778    ...     0.032649  0.033395  0.032567   \n",
       "0   0.033364  0.033468  0.032888    ...     0.032812  0.033317  0.032587   \n",
       "0   0.033301  0.033536  0.033103    ...     0.032634  0.033184  0.032408   \n",
       "0   0.033166  0.033656  0.032807    ...     0.032603  0.033226  0.032559   \n",
       "0   0.033371  0.033722  0.032878    ...     0.032434  0.033345  0.032284   \n",
       "0   0.033172  0.033576  0.033205    ...     0.032740  0.033184  0.032493   \n",
       "0   0.033526  0.033531  0.033075    ...     0.032524  0.032943  0.032614   \n",
       "0   0.033484  0.033390  0.032975    ...     0.032580  0.033222  0.032656   \n",
       "0   0.033403  0.033556  0.033161    ...     0.033118  0.033524  0.032741   \n",
       "0   0.033177  0.033539  0.032751    ...     0.032544  0.033301  0.032283   \n",
       "0   0.033209  0.033540  0.032970    ...     0.032775  0.034182  0.032603   \n",
       "0   0.033282  0.033642  0.033088    ...     0.033178  0.033677  0.032920   \n",
       "0   0.033045  0.033669  0.032604    ...     0.032609  0.033314  0.032640   \n",
       "0   0.033126  0.033707  0.033008    ...     0.032882  0.033572  0.032881   \n",
       "0   0.033157  0.033406  0.033045    ...     0.033139  0.033461  0.032764   \n",
       "0   0.033027  0.033708  0.032946    ...     0.032588  0.033658  0.032211   \n",
       "0   0.033165  0.033714  0.033002    ...     0.032998  0.033401  0.033060   \n",
       "0   0.033208  0.033630  0.033080    ...     0.032987  0.033482  0.032790   \n",
       "..       ...       ...       ...    ...          ...       ...       ...   \n",
       "0   0.033616  0.033261  0.032813    ...     0.032539  0.033435  0.032753   \n",
       "0   0.033101  0.033547  0.032772    ...     0.032713  0.033363  0.032608   \n",
       "0   0.033509  0.033665  0.033097    ...     0.032465  0.033673  0.032848   \n",
       "0   0.033632  0.033770  0.032980    ...     0.032529  0.033012  0.032112   \n",
       "0   0.033033  0.033494  0.032991    ...     0.032815  0.033307  0.032695   \n",
       "0   0.033167  0.033131  0.032863    ...     0.032653  0.032930  0.032660   \n",
       "0   0.033295  0.033286  0.033210    ...     0.032792  0.032839  0.032431   \n",
       "0   0.033295  0.033551  0.033105    ...     0.032829  0.033406  0.032843   \n",
       "0   0.033203  0.033510  0.033098    ...     0.032590  0.033049  0.032562   \n",
       "0   0.033084  0.033515  0.032796    ...     0.032871  0.032957  0.032588   \n",
       "0   0.032749  0.033823  0.032750    ...     0.032514  0.033687  0.032435   \n",
       "0   0.033401  0.033579  0.032839    ...     0.032558  0.033323  0.032517   \n",
       "0   0.033227  0.033485  0.033053    ...     0.032733  0.033331  0.032632   \n",
       "0   0.033204  0.033686  0.032866    ...     0.032795  0.033426  0.032689   \n",
       "0   0.033453  0.034006  0.033197    ...     0.032527  0.033586  0.032392   \n",
       "0   0.033345  0.033500  0.032933    ...     0.032688  0.033401  0.032576   \n",
       "0   0.033118  0.033619  0.032858    ...     0.032641  0.033343  0.032724   \n",
       "0   0.033399  0.033583  0.033082    ...     0.032534  0.033290  0.032474   \n",
       "0   0.033255  0.033729  0.033011    ...     0.032894  0.033428  0.032683   \n",
       "0   0.033628  0.033445  0.032857    ...     0.032257  0.033275  0.032424   \n",
       "0   0.033722  0.033571  0.032832    ...     0.032464  0.033178  0.032345   \n",
       "0   0.033305  0.033702  0.032689    ...     0.032632  0.033506  0.032731   \n",
       "0   0.033125  0.033642  0.032644    ...     0.032700  0.033162  0.032604   \n",
       "0   0.033000  0.033674  0.032916    ...     0.032754  0.033207  0.032863   \n",
       "0   0.032959  0.033569  0.032994    ...     0.033008  0.033090  0.032379   \n",
       "0   0.033016  0.034017  0.032490    ...     0.032262  0.033435  0.032358   \n",
       "0   0.033324  0.033375  0.033183    ...     0.032889  0.033754  0.032889   \n",
       "0   0.033543  0.033861  0.032715    ...     0.032509  0.033245  0.032642   \n",
       "0   0.033190  0.033550  0.032702    ...     0.032721  0.033457  0.032664   \n",
       "0   0.033372  0.033448  0.033151    ...     0.032649  0.033506  0.032836   \n",
       "\n",
       "          23        24        25        26        27        28        29  \n",
       "0   0.032708  0.032900  0.033168  0.033620  0.033144  0.033960  0.033615  \n",
       "0   0.032496  0.032889  0.033744  0.033812  0.033280  0.034002  0.033667  \n",
       "0   0.032654  0.032883  0.033431  0.033545  0.033342  0.034032  0.033689  \n",
       "0   0.032393  0.032971  0.033410  0.033707  0.033168  0.033798  0.033470  \n",
       "0   0.032894  0.032852  0.033465  0.033699  0.033268  0.034034  0.033587  \n",
       "0   0.032193  0.032978  0.033566  0.033556  0.033072  0.034136  0.033647  \n",
       "0   0.032545  0.032726  0.033869  0.033384  0.033059  0.034323  0.033889  \n",
       "0   0.032945  0.033061  0.033794  0.033285  0.033424  0.033861  0.033353  \n",
       "0   0.032872  0.033149  0.033420  0.033400  0.033513  0.033924  0.033613  \n",
       "0   0.032441  0.032666  0.033418  0.033619  0.033298  0.033935  0.033341  \n",
       "0   0.032518  0.032684  0.033525  0.033554  0.033213  0.034342  0.033784  \n",
       "0   0.032912  0.032726  0.033941  0.033561  0.033183  0.034041  0.033713  \n",
       "0   0.032640  0.032641  0.033616  0.033365  0.033293  0.033969  0.033738  \n",
       "0   0.032647  0.032696  0.033487  0.033506  0.033277  0.034052  0.033700  \n",
       "0   0.032468  0.032884  0.033265  0.033711  0.033134  0.033996  0.033765  \n",
       "0   0.032669  0.032879  0.033368  0.033571  0.033143  0.033994  0.033849  \n",
       "0   0.032527  0.032678  0.033500  0.033474  0.033200  0.034229  0.033755  \n",
       "0   0.032805  0.033067  0.033350  0.033500  0.033141  0.033811  0.033692  \n",
       "0   0.032664  0.032680  0.033595  0.033738  0.033281  0.034072  0.033491  \n",
       "0   0.032510  0.033027  0.033532  0.033622  0.033162  0.034032  0.033724  \n",
       "0   0.033035  0.032989  0.033087  0.033367  0.033206  0.033693  0.033521  \n",
       "0   0.032467  0.032892  0.033458  0.033627  0.033140  0.034125  0.033789  \n",
       "0   0.033177  0.032548  0.033457  0.033623  0.033291  0.034120  0.033454  \n",
       "0   0.033015  0.032898  0.033535  0.033453  0.033390  0.033761  0.033248  \n",
       "0   0.032444  0.032836  0.033749  0.033321  0.033233  0.033941  0.033605  \n",
       "0   0.032757  0.032651  0.033248  0.033496  0.033369  0.033801  0.033395  \n",
       "0   0.033007  0.032929  0.033440  0.033554  0.033192  0.033659  0.033563  \n",
       "0   0.032275  0.032883  0.033527  0.033578  0.033191  0.034144  0.033602  \n",
       "0   0.032947  0.032901  0.033496  0.033205  0.033528  0.033619  0.033586  \n",
       "0   0.032505  0.032520  0.033566  0.033582  0.033544  0.034012  0.033252  \n",
       "..       ...       ...       ...       ...       ...       ...       ...  \n",
       "0   0.032688  0.032833  0.033602  0.033631  0.033288  0.033967  0.033687  \n",
       "0   0.032665  0.032758  0.033696  0.033388  0.033179  0.033827  0.033746  \n",
       "0   0.032472  0.032929  0.033184  0.033527  0.033327  0.033991  0.033276  \n",
       "0   0.032245  0.032663  0.033626  0.033655  0.033299  0.034367  0.034055  \n",
       "0   0.032878  0.033075  0.033651  0.033290  0.033246  0.033639  0.033348  \n",
       "0   0.032676  0.033059  0.033930  0.033698  0.033285  0.033942  0.033739  \n",
       "0   0.032554  0.033044  0.033674  0.033589  0.033247  0.033978  0.033905  \n",
       "0   0.032809  0.033009  0.033500  0.033381  0.033226  0.033824  0.033537  \n",
       "0   0.032904  0.033023  0.033686  0.033488  0.033664  0.033649  0.033981  \n",
       "0   0.032855  0.032542  0.033541  0.033726  0.033266  0.034224  0.033815  \n",
       "0   0.032446  0.032807  0.033670  0.033542  0.033316  0.034195  0.033748  \n",
       "0   0.032651  0.032920  0.033450  0.033488  0.033437  0.033938  0.033569  \n",
       "0   0.032801  0.033183  0.033876  0.033120  0.033257  0.033739  0.033233  \n",
       "0   0.032717  0.032950  0.033113  0.033634  0.033381  0.034018  0.033737  \n",
       "0   0.032331  0.032764  0.033303  0.033569  0.033429  0.033912  0.033405  \n",
       "0   0.032909  0.033100  0.033589  0.033578  0.033232  0.033756  0.033620  \n",
       "0   0.032445  0.032822  0.033537  0.033538  0.033099  0.034132  0.033836  \n",
       "0   0.032234  0.032950  0.033280  0.033771  0.033611  0.033986  0.033659  \n",
       "0   0.033018  0.032988  0.033374  0.033271  0.033167  0.033766  0.033551  \n",
       "0   0.032393  0.032773  0.033484  0.033914  0.033176  0.034167  0.033929  \n",
       "0   0.032546  0.032837  0.033338  0.033694  0.033268  0.033954  0.033968  \n",
       "0   0.032626  0.032960  0.033188  0.033594  0.033207  0.033842  0.033609  \n",
       "0   0.032811  0.032588  0.033530  0.033462  0.033425  0.033970  0.033718  \n",
       "0   0.032607  0.032850  0.033557  0.033405  0.033268  0.033889  0.033527  \n",
       "0   0.032323  0.032837  0.034088  0.033799  0.033131  0.034290  0.033588  \n",
       "0   0.032240  0.032577  0.033760  0.033561  0.033366  0.034235  0.033918  \n",
       "0   0.032772  0.032868  0.033138  0.033360  0.033367  0.033950  0.033557  \n",
       "0   0.032481  0.032862  0.033624  0.033438  0.033046  0.034178  0.033542  \n",
       "0   0.032622  0.032992  0.033651  0.033515  0.033170  0.034242  0.033794  \n",
       "0   0.032947  0.033060  0.033407  0.033337  0.033150  0.033787  0.033483  \n",
       "\n",
       "[256 rows x 30 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appended_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
